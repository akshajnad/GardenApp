#############################################
# train_advanced_schedule_model.ipynb
#############################################

!pip install pandas numpy scikit-learn tensorflow transformers sentencepiece

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

from transformers import GPT2Tokenizer, TFGPT2LMHeadModel

# 1) Load the final dataset
df = pd.read_csv("final_dataset.csv")

# Some cleaning
df = df.dropna(subset=["text_instructions"])
df = df.fillna(0)

# 2) Split numeric vs text
numeric_cols = [
    "urgency", "num_people", "volume_goal", "calorie_goal",
    "free_space", "weather_temp", "weather_rain", "existing_crops_vector"
]
label_num_col = "fraction_of_space_used"
text_col = "text_instructions"

X_num = df[numeric_cols].values
y_num = df[label_num_col].values.reshape(-1,1)  # fraction_of_space
text_data = df[text_col].tolist()

# 3) Train/val split
X_train_num, X_val_num, y_train_num, y_val_num, text_train, text_val = train_test_split(
    X_num, y_num, text_data, test_size=0.2, random_state=42
)

########################################
# A) Numeric Model (Allocation)
########################################
input_dim = X_train_num.shape[1]
inp = tf.keras.Input(shape=(input_dim,))
x = layers.Dense(128, activation='relu')(inp)
x = layers.Dense(64, activation='relu')(x)
frac_out = layers.Dense(1, activation='sigmoid')(x)  # fraction_of_space

allocation_model = tf.keras.Model(inputs=inp, outputs=frac_out)
allocation_model.compile(
    optimizer='adam',
    loss='mse',
    metrics=['mae']
)
allocation_model.summary()

history_alloc = allocation_model.fit(
    X_train_num, y_train_num,
    validation_data=(X_val_num, y_val_num),
    epochs=20,
    batch_size=16
)

########################################
# B) Text Model (GPT-2)
########################################
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # gpt2 doesn't have real pad token

train_encodings = tokenizer(text_train, truncation=True, padding=True, max_length=256)
val_encodings = tokenizer(text_val, truncation=True, padding=True, max_length=256)

train_input_ids = np.array(train_encodings['input_ids'])
train_attn_masks = np.array(train_encodings['attention_mask'])

val_input_ids = np.array(val_encodings['input_ids'])
val_attn_masks = np.array(val_encodings['attention_mask'])

def map_fn(input_ids, attn_mask):
    return {"input_ids": input_ids, "attention_mask": attn_mask}, input_ids

train_dataset = tf.data.Dataset.from_tensor_slices((train_input_ids, train_attn_masks))
train_dataset = train_dataset.map(map_fn).batch(2)

val_dataset = tf.data.Dataset.from_tensor_slices((val_input_ids, val_attn_masks))
val_dataset = val_dataset.map(map_fn).batch(2)

gpt_model = TFGPT2LMHeadModel.from_pretrained("gpt2")

gpt_model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-5),
    loss=gpt_model.compute_loss
)

history_gpt = gpt_model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=3
)

########################################
# SAVE BOTH MODELS
########################################
allocation_model.save("allocation_model.h5")
gpt_model.save_pretrained("./text_generation_model")
tokenizer.save_pretrained("./text_generation_model")

print("All models saved. Training complete!")
